# 事务处理

事务的概念起源于数据库系统，但是现在已经不再局限于数据库本身了。

原子性、隔离性、持久性是手段，一致性是目的。事务一致性可细分为：

- 内部一致性：一个服务使用一个数据源；
- 外部一致性：一个服务使用多个数据源。

事务的几个阶段可分为：开启、终止、提交、回滚、嵌套、设置隔离级别。

数据源：指提供数据的逻辑设备，不必与物理设备一一对应。

## 本地事务

适用场景：一个服务使用一个数据源。

### 实现原子性

实现原子性的最大障碍是：写入磁盘操作不是原子的。因为有 “正在写” 的中间状态。

- 未提交事务，写入后崩溃：撤销磁盘操作；
- 已提交事务，写入前崩溃：重新写入磁盘。

为了能够崩溃恢复，主流方式是使用 **提交日志（Commit Logging）** 的方法，步骤如下：

- 将用户操作按照顺序追加的方式，写到日志文件中；
- 数据库看到了提交记录（Commit Record）；
- 根据日志修改数据，并在日志中添加结束记录（End Record）。

另一种方法是 **影子分页（Shadow Paging）** 应用案例如 SQLite Version 3。
基本思路是先复制一份副本，保留原数据，修改副本，修改完成后修改指针。
影子分页比提交日志简单，但是涉及隔离性与并发锁时，性能较低，因此在高性能数据库中较少使用。

提交日志的方法的**局限性**：所有对数据的真实修改都必须发生在事务提交后，即使磁盘 I/O 有空闲。

改进方法： **提前写入日志（Write-Ahead Logging）** 。它分两种情况：

- FORCE：当事务提交后，要求变动数据必须同时完成写入。相应地，不要求同时变动数据为 NO-FORCE。
- STEAL：在提交事务前，允许变动数据提前写入。相应地，不允许变动提前写入为 NO-STEAL。

提交日志方法允许 NO-FORCE 但不允许 STEAL。提前写入日志允许 NO-FORCE 也允许 STEAL，
它的解决方法是引入回滚日志（Undo Log）的日志类型，当变动写入磁盘前，需要先记录回滚日志。
此前记录的日志为重做日志（Redo Log）。
[这两个日志有什么不一样？](https://www.cnblogs.com/f-ck-need-u/p/9010872.html)

- 回滚日志：用于擦除提前写入的变动；
- 重做日志：用于重演数据变动。

提前写入日志方法在崩溃恢复时经历以下三个阶段：

- **分析阶段**：从最后一次检查点（Checkpoint）扫描日志，找出所有没有 End Record 的事务，组成待恢复的事务集合。
  这个集合至少包含事务表（Transaction Table）和脏页表（Dirty Page Table）；
- **重做阶段**：将待恢复的事务集合中包含 Commit Record 的事务重新写入磁盘。完成后写入 End Record；
- **回滚阶段**：处理待恢复的事务集合中的剩余事务，将提前写入的变动改回原样。

### 实现隔离性

隔离性保证各个事务的读、写互相独立，不会彼此影响。实现隔离性的方法是加锁同步：

- **写锁（Write Lock，X-Lock）**：也叫排它锁。
  只能有一个事务持有数据的写锁，其他事务不能再对该数据加读锁。
- **读锁（Read Lock，S-Lock）**：也叫共享锁。
  多个事务可以对同一个数据加多个读锁。如果只有一个事务持有读锁，可以将读锁升级为写锁。
- **范围锁（Range Lock）**：选中一个范围的数据，施加排它锁。
  对该范围内的数据不能新增或删除，因此它不等价于一组排它锁。

隔离性分为四个级别，隔离程度越高，并发访问的吞吐量就越低：

- **可串行化**：对事务涉及的数据加读锁、写锁、范围锁。其中加锁和解锁两个阶段称为两阶段锁。
- **可重复读**：对事务涉及的数据加读锁、写锁，且一直持续到事务结束，但不加范围锁。
  有幻读问题：对某个范围的两次查询结果不一样。此为 MySQL/InnoDB 的默认隔离级别。
- **读已提交**：对事务涉及的数据加读锁、写锁，写锁一直持续到事务结束，读锁在查询操作完成后马上释放。
  有不可重复读问题：对同一行数据的两次查询结果不同。
- **读未提交**：只对事务涉及的数据加写锁，且一直持续到事务结束。
  有脏读问题：在事务执行过程中，一个事务读取到了另一个事务没提交的数据。
  注意，写锁禁止其他事务施加读锁，而不是禁止事务读取数据。

以上幻读、不可重复读、脏读等问题都是由于一个事务在读数据时，受另一个写数据的事务影响而破坏了隔离性。

针对这种 “读 + 写” 的隔离问题，有一个称为 “**多版本并发控制（MVCC）**” 的无锁优化方案被主流商业数据库厂商使用。
MVCC 是一种读取优化策略，它的 “无锁” 是指读取时不需要加锁。
基本思路是对数据库的任何修改都不会直接覆盖之前的数据，而是产生一个新版本与老版本共存。
实现方法如下：

- 创建两个隐藏字段：`CREATE_VERSION`、`DELETE_VERSION`，两个字段记录的都是事务 ID；
- 插入数据时，`CREATE_VERSION` 记录插入数据的事务 ID，`DELETE_VERSION` 为空；
- 删除数据时，`CREATE_VERSION` 为空，`DELETE_VERSION` 记录删除数据的事务 ID；
- 修改数据时，先将原有的数据复制一份，删除原有数据，插入复制数据。

然后，根据隔离级别选择应该读取哪个版本的数据：

- 隔离级别是可重复读：总是读取 `CREATE_VERSION` ≤ 当前事务 ID 的记录；
- 隔离级别是读已提交：总是读取最新版本，即最近被提交的版本的数据记录；
- 隔离级别是读未提交：不用 MVCC，直接修改原始数据即可；
- 隔离级别是可串行化：不用 MVCC，可串行化会阻塞其他事务，与 MVCC 相悖。

由于 MVCC 只是针对 “读 + 写” 场景的优化，如果是 “写 + 写” 的场景，加锁几乎是唯一的方法。
可细分为：

- 乐观加锁：认为数据存在竞争是偶然现象，不应该一开始就加锁。数据竞争激烈时效率较低；
- 悲观加锁：前面所述都是悲观加锁方案。

因此，对于隔离性而言，也没有一个十全十美的方案。

## 全局事务

适用场景：一个服务使用多个数据源。

理论上没有单个服务的约束，本来就是 DTP（分布式事务处理）模型中的概念。

为了解决分布式事务的一致性问题，X/Open 组织提出了 X/Open XA 处理事务架构。
其核心内容是定义了全局的**事务管理器**和局部的**资源管理器**之间的通信接口。

XA 接口是双向的，能在一个事务管理器和多个资源管理器之间形成通信桥梁，
通过协调多个数据源的一致动作，实现全局事务的统一提交或者统一回滚。

XA 将事务提交拆分为两个阶段：准备阶段和提交阶段。

```{mermaid}
sequenceDiagram
    协调者 ->>+ 参与者: 要求所有参与者进入准备阶段
    参与者 -->>- 协调者: 已进入准备阶段
    协调者 ->>+ 参与者: 要求所有参与者进入提交阶段
    参与者 -->>- 协调者: 已进入提交阶段
    opt 失败或超时
        协调者 ->>+ 参与者: 要求所有参与者回滚事务
        参与者 -->>- 协调者: 已回滚事务
    end
```

以上被称为 “两段式提交（2PC）” 协议，而它能够成功保持一致性需要满足以下前提：

- 必须假设网络在提交阶段的短时间内是可靠的，即提交阶段不会丢失消息。
- 必须假设因为网络分区、机器崩溃或者其他原因而导致失联的节点最终能够恢复，不会永久性地处于失联状态。

两段式提交原理简单，但是有几个显著缺点：

- 单点问题：主要是由于协调者的中心决策；
- 性能问题：两次远程服务调用、三次数据持久化、木桶效应；
- 一致性风险：网络故障、服务器宕机。

改进方法：三段式提交（3PC）协议，将准备阶段分为 CanCommit 和 PreCommit，提交阶段改称 DoCommit。
因为准备阶段如果发生失败，回滚的代价是昂贵的。但是在正常提交的场景下，两者的性能都很差，3PC 更差。

```{mermaid}
sequenceDiagram
    协调者 ->>+ 参与者: 询问阶段：是否有把握完成事务
    参与者 -->>- 协调者: 是
    协调者 ->>+ 参与者: 准备阶段：写入日志，锁定资源
    参与者 -->>- 协调者: 确认（Ack）
    协调者 ->>+ 参与者: 提交阶段：提交事务
    参与者 -->>- 协调者: 已提交
    opt 失败
        协调者 ->>+ 参与者: 要求回滚
        参与者 -->>- 协调者: 已回滚
    end
    opt 超时
        参与者 ->> 参与者: 提交事务
    end
```

3PC 对 2PC 的单点问题和回滚时的性能问题有所改善，但是对一致性风险问题没有任何改进，甚至略有增加。
因为，超时后参与者仍会提交事务，这时有可能会产生多个参与者之间的数据不一致。

## 共享事务

适用场景：多个服务共享一个数据源。

理论可行的方案：直接让各个服务共享数据库连接。但是实际不可行，因为数据库连接的基础是网络连接，
它是与 IP 地址和端口号绑定的，字面意义上的 “不同服务节点上共享数据库连接” 很难做到。
因此，为了实现共享事务，必须新增 “**中间服务器**” ，将它作为各个服务的**远程数据库连接池**来看待。
然后由中间服务器与数据库连接，这就相当于一个本地事务了。

```{mermaid}
graph LR
    User("用户账户") --> Proxy("交易服务器")
    Business("商家账户") --> Proxy
    Warehouse("商品仓库") --> Proxy
    Proxy --> Database("数据库 ")
```

**这种方式在现实中是不可用的。**
因为实际生产系统中，我们会为多个数据库实例使用负载均衡，但**通常不会**对多个服务做负载均衡。
这种方法在实际应用中**并不值得提倡**，鲜有采用这种方式的成功案例。

也就是说，共享事务一般不会再现实中出现。

## 分布式事务

使用场景：多个服务使用多个数据源。

本节所说的分布式有别于 DTP 模型中的分布式。DTP 模型中的分布式是针对多个数据源来说的，不涉及服务。
本节说的分布式针对的是多个服务。

CAP 理论：

- 一致性（C）：在任何时刻、任何分布式节点中所看到的都是符合预期的；
- 可用性（A）：系统不间断地提供服务的能力；
- 分区容忍性（P）：节点之间形成 “网络分区” 时，系统仍能正确地提供服务的能力。

三个特性只能同时满足两个：

- CA without P：放弃分区容忍性。通过共享存储来保证没有网络分区，如 Oracle RAC。
- CP without A：放弃可用性。以时间为代价，保证一致性。如 HBase 集群。
- AP without C：放弃一致性。通过本地缓存保证可用性。分布式系统的主流选择。如 NoSQL 和 Redis 集群。

凤凰架构的服务拓扑结构如下图所示：

```{mermaid}
graph TB
    User("最终用户")-->Store("Fenix's Bookstore")
    Store-->Warehouse("仓库服务集群")
    Store-->Business("商家服务集群")
    Store-->Account("账号服务集群")
    
    subgraph o1
        Warehouse-.->Warehouse1("仓库节点1")
        Warehouse-.->Warehouse2("仓库节点2")
        Warehouse-->WarehouseN("仓库节点N")
    end
    subgraph o2
        Business-.->Business1("商家节点1")
        Business-->Business2("商家节点2")
        Business-.->BusinessN("商家节点N")
    end
    subgraph o3
        Account-->Account1("账号节点1")
        Account-.->Account2("账号节点2")
        Account-.->AccountN("账号节点N")
    end
```

每个节点都有自己的数据库，一次交易需要由多个节点联合响应。
比如，当用户购买一件价值 100 元的商品后：

- 账号节点需要扣款 100 元；
- 商家节点收款 100 元；
- 仓库节点发货。

### 可靠事件队列

事件队列通过轮询（不断重试，也叫最大努力交付）的方式保证事务的准确性。

工具有 RocketMQ。

```{mermaid}
sequenceDiagram
    Fenix's Bookstore ->>+ 账号服务: 启动事务
    账号服务 ->> 账号服务: 扣减货款
    账号服务 ->>- 消息队列: 提交本地事务，发出消息
    loop 循环直至成功
        消息队列 ->> 仓库服务: 扣减库存
        alt 扣减成功
            仓库服务 -->> 消息队列: 成功
        else 业务或网络异常
            仓库服务 -->> 消息队列: 失败
        end
    end
    消息队列 -->> 账号服务: 更新消息表，仓库服务完成
    loop 循环直至成功
        消息队列 ->> 商家服务: 货款收款
        alt 收款成功
            商家服务 -->> 消息队列: 成功
        else 业务或网络异常
            商家服务 -->> 消息队列: 失败
        end
    end
    消息队列 -->> 账号服务: 更新消息表，商家服务完成
```

可靠消息队列保证了最终结果的一致性，但是却没有任何隔离性可言。

### TCC 事务

对于需要隔离性的事务，需要使用 TCC 事务，它包含 Try、Confirm、Cancel 三个阶段。
比如下图所示的 “超售” 案例，需要可重复读的隔离级别。

```{mermaid}
sequenceDiagram
    Fenix's Bookstore ->> 账号服务: 业务检查，冻结货款
    alt 成功
        账号服务 -->> Fenix's Bookstore: 记录进入Confirm阶段
    else 业务或网络异常
        账号服务 -->> Fenix's Bookstore: 记录进入Cancel阶段
    end
    Fenix's Bookstore ->> 仓库服务: 业务检查，冻结商品
    alt 成功
        仓库服务 -->> Fenix's Bookstore: 记录进入Confirm阶段
    else 业务或网络异常
        仓库服务 -->> Fenix's Bookstore: 记录进入Cancel阶段
    end
    Fenix's Bookstore ->> 商家服务: 业务检查
    alt 成功
        商家服务 -->> Fenix's Bookstore: 记录进入Confirm阶段
    else 业务或网络异常
        商家服务 -->> Fenix's Bookstore: 记录进入Cancel阶段
    end
    opt 全部记录均返回Confirm阶段
        loop 循环直至全部成功
            Fenix's Bookstore->>账号服务: 完成业务，扣减冻结的货款
            Fenix's Bookstore->>仓库服务: 完成业务，扣减冻结的货物
            Fenix's Bookstore->>商家服务: 完成业务，货款收款
        end
    end
    opt 任意服务超时或返回Cancel阶段
        loop 循环直至全部成功
            Fenix's Bookstore->>账号服务:取消业务，解冻货款
            Fenix's Bookstore->>仓库服务:取消业务， 解冻货物
            Fenix's Bookstore->>商家服务:取消业务
        end
    end
```

缺点：TCC 事务的业务侵入性较强，要求业务处理过程必须拆分为 “预留业务资源” 和 “确认/释放消费资源” 两个子过程。

但是 TCC 事务在柔性事务中，性能还是比较高的（相对 SAGA 事务来讲）。

通常我们不会完全靠裸编码来实现 TCC，而是基于某些分布式事务中间件（如阿里开源的 [Seata](https://seata.io/zh-cn/)）去完成，尽量减轻一些编码工作量。

### SAGA 事务

跨越两个系统的事务无法在 TCC 事务中实现。比如 “使用第三方支付”，我们无权对银行的数据库进行干涉。

SAGA 事务的主要思路如下：

- 将大事务拆分若干个小事务，将整个分布式事务 $T$ 分解为 $n$ 个子事务，命名为 $T_1, T_2, ..., T_i, ..., T_n$。
  每个子事务都应该是或者能被视为是原子行为。
  如果分布式事务能够正常提交，其对数据的影响（最终一致性）应与连续按顺序成功提交 $T_i$ 等价。
- 为每一个子事务设计对应的补偿动作，命名为 $C_1, C_2, ..., C_i, ..., C_n$。$T_i$ 与 $C_i$ 必须满足以下条件：

  - $T_i$ 与 $C_i$ 都具备幂等性。
  - $T_i$ 与 $C_i$ 满足交换律（Commutative），即先执行 $T_i$ 还是先执行 $C_i$，其效果都是一样的。
  - $C_i$ **必须能成功提交**，即不考虑 $C_i$ 本身提交失败被回滚的情形，如出现就必须持续重试直至成功，或者要人工介入。

如果 $T_1$ 到 $T_n$ 均成功提交，那事务顺利完成，否则，要采取以下两种**恢复策略**之一：

- 正向恢复（Forward Recovery）：如果 $T_i$ 事务提交失败，则一直对 $T_i$ 进行重试，直至成功为止（**最大努力交付**）。
  这种恢复方式不需要补偿，适用于事务最终都要成功的场景，譬如在别人的银行账号中扣了款，就一定要给别人发货。
  
  - 正向恢复的执行模式为：$T_1, T_2, ..., T_i（失败）, T_i（重试）..., T_{i+1}, ..., T_n$。

- 反向恢复（Backward Recovery）：如果 $T_i$ 事务提交失败，则一直执行 $C_i$ 对 $T_i$ 进行补偿，直至成功为止（**最大努力交付**）。
  这里要求 $C_i$ 必须（在持续重试后）执行成功。
  
  - 反向恢复的执行模式为：$T_1, T_2, ..., T_i（失败）, C_i（补偿）..., C_{i-1}, ..., C_2, C_1$。

与 TCC 相比，SAGA 不需要为资源设计冻结状态和撤销冻结的操作，*补偿操作往往要比冻结操作容易实现得多*。

SAGA 系统本身也有可能会崩溃，所以它必须设计与数据库类似的日志机制（被称为 SAGA Log），以保证系统恢复后可以追踪到子事务的执行情况。
譬如执行至哪一步或者补偿至哪一步了。

SAGA 事务实现起来也不太容易，通常也不会直接靠裸编码来实现，一般也是在事务中间件的基础上完成，Seata 同样支持 SAGA 事务模式。
